{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab72bfb",
   "metadata": {},
   "source": [
    "# PySpark basics and getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea605f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5dd183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hinat</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suki</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>momo</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0  hinat   31\n",
       "1   suki   30\n",
       "2   momo   29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"test1.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c207d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # create pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e5ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c313081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2KVSSK2L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f086109630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24183d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5ff33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b73996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  _c0|_c1|\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ca5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028dc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0900ae61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='hinat', Age='31'),\n",
       " Row(Name='suki', Age='30'),\n",
       " Row(Name='momo', Age='29')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ab83e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee61a9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5128d",
   "metadata": {},
   "source": [
    "# PySpark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beb4fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d0e9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "23f34b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv', sep = ';', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b57a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "598fe835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f51372ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';').show()\n",
    "spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e8674bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32b52945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='hinat', Age=31), Row(Name='suki', Age=30), Row(Name='momo', Age=29)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5fa5e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## selecting a column\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "179978c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|hinat|\n",
      "| suki|\n",
      "| momo|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7bcbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c87fa52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cee570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+\n",
      "|summary| Name| Age|\n",
      "+-------+-----+----+\n",
      "|  count|    3|   3|\n",
      "|   mean| null|30.0|\n",
      "| stddev| null| 1.0|\n",
      "|    min|hinat|  29|\n",
      "|    max| suki|  31|\n",
      "+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95603ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding columns in dataframe\n",
    "df_pyspark = df_pyspark.withColumn('Exp', df_pyspark['Age']+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5911a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| Name|Age|Exp|\n",
      "+-----+---+---+\n",
      "|hinat| 31| 41|\n",
      "| suki| 30| 40|\n",
      "| momo| 29| 39|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d6d61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop columns\n",
    "df_pyspark = df_pyspark.drop('Exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e616cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca8d1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|Fname|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### rename columns\n",
    "df_pyspark.withColumnRenamed('Name', 'Fname').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "682018b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fe050",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01011985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5b722f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Missing value').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "da8b13e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', sep = ';', header = True, inferSchema = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c1c3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "| Age| Exp|Salary|\n",
      "+----+----+------+\n",
      "|  31|  10| 30000|\n",
      "|  30|   8| 25000|\n",
      "|  29|   4| 20000|\n",
      "|  24|   3| 20000|\n",
      "|  21|   1| 15000|\n",
      "|  23|   2| 18000|\n",
      "|null|null| 40000|\n",
      "|  34|  10| 38000|\n",
      "|  36|null|  null|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6f63e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f03edc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c895082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## any == how\n",
    "df_pyspark.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "73a5c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold of 2 \n",
    "df_pyspark.na.drop(how = 'any', thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "168e40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "| null| 23|  2| 18000|\n",
      "| null| 34| 10| 38000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## subset - drops null values from 'Exp'\n",
    "df_pyspark.na.drop(how = 'any', subset = ['Exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2fc752e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+------+\n",
      "|         Name| Age| Exp|Salary|\n",
      "+-------------+----+----+------+\n",
      "|        hinat|  31|  10| 30000|\n",
      "|         suki|  30|   8| 25000|\n",
      "|         momo|  29|   4| 20000|\n",
      "|          zum|  24|   3| 20000|\n",
      "|         maki|  21|   1| 15000|\n",
      "|Missing value|  23|   2| 18000|\n",
      "|          avo|null|null| 40000|\n",
      "|Missing value|  34|  10| 38000|\n",
      "|Missing value|  36|null|  null|\n",
      "+-------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### fill missing value\n",
    "df_pyspark.na.fill({'Name': 'Missing value'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e2cdedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e40e56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Exp', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Exp', 'Salary']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4da0501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "| Name| Age| Exp|Salary|Age_imputed|Exp_imputed|Salary_imputed|\n",
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "|hinat|  31|  10| 30000|         31|         10|         30000|\n",
      "| suki|  30|   8| 25000|         30|          8|         25000|\n",
      "| momo|  29|   4| 20000|         29|          4|         20000|\n",
      "|  zum|  24|   3| 20000|         24|          3|         20000|\n",
      "| maki|  21|   1| 15000|         21|          1|         15000|\n",
      "| null|  23|   2| 18000|         23|          2|         18000|\n",
      "|  avo|null|null| 40000|         28|          5|         40000|\n",
      "| null|  34|  10| 38000|         34|         10|         38000|\n",
      "| null|  36|null|  null|         36|          5|         25750|\n",
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "549916b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf852507",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d3130733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Filter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8c7a3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# got tired of using df_pyspark\n",
    "df = spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b3da22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0da6b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|suki| 30|\n",
      "|momo| 29|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## cats age less than 30\n",
    "df.filter(\"Age<=30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "32498a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|suki|\n",
      "|momo|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Age<=30\").select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe3876a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df['Age']<=30) & (df['Name'] == 'suki')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "242b798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F091747BE0>>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f67769",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99fbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Group').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9970b2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2KVSSK2L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Group</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a216c4c4c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd49e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('test3.csv', header = True, inferSchema = True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba6045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+\n",
      "| Name|Departments|Salary|\n",
      "+-----+-----------+------+\n",
      "|hinat|         DS| 10000|\n",
      "| suki|        IOT|  5000|\n",
      "| momo|         BD|  4000|\n",
      "|  zum|         BD|  4000|\n",
      "| maki|         DS|  3000|\n",
      "|bussy|         DS| 20000|\n",
      "|  avo|        IOT| 10000|\n",
      "|Sunny|         BD|  5000|\n",
      "|Sunny|         DS| 10000|\n",
      "|Krish|         BD|  2000|\n",
      "+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30704bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d361703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|sum(Salary)|\n",
      "+-----+-----------+\n",
      "| suki|       5000|\n",
      "|Sunny|      15000|\n",
      "|  zum|       4000|\n",
      "|Krish|       2000|\n",
      "|bussy|      20000|\n",
      "| maki|       3000|\n",
      "| momo|       4000|\n",
      "|hinat|      10000|\n",
      "|  avo|      10000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "df.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dae1f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|Departments|sum(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOT|      15000|\n",
      "|         BD|      15000|\n",
      "|         DS|      43000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby departments\n",
    "df.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "890153e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|Departments|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOT|     7500.0|\n",
      "|         BD|     3750.0|\n",
      "|         DS|    10750.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e4360bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|Departments|count|\n",
      "+-----------+-----+\n",
      "|        IOT|    2|\n",
      "|         BD|    4|\n",
      "|         DS|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e133c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "704b77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|min(Salary)|\n",
      "+-----+-----------+\n",
      "| suki|       5000|\n",
      "|Sunny|       5000|\n",
      "|  zum|       4000|\n",
      "|Krish|       2000|\n",
      "|bussy|      20000|\n",
      "| maki|       3000|\n",
      "| momo|       4000|\n",
      "|hinat|      10000|\n",
      "|  avo|      10000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8989b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
