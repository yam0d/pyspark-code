{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf12dc8",
   "metadata": {},
   "source": [
    "# PySpark basics and getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea605f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5dd183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hinat</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suki</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>momo</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0  hinat   31\n",
       "1   suki   30\n",
       "2   momo   29"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"test1.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c207d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # create pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee73a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c313081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2KVSSK2L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f086109630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24183d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd59349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec724182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  _c0|_c1|\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f55f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44e56ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f803275c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='hinat', Age='31'),\n",
       " Row(Name='suki', Age='30'),\n",
       " Row(Name='momo', Age='29')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "916ba310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d27e8e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1d169",
   "metadata": {},
   "source": [
    "# PySpark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3317a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e4beac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3656df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv', sep = ';', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "448ced03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a336ad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32702fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';').show()\n",
    "spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e53b00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae5d678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='hinat', Age=31), Row(Name='suki', Age=30), Row(Name='momo', Age=29)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b1efef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## selecting a column\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be479dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|hinat|\n",
      "| suki|\n",
      "| momo|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86f065b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f34795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "911db8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+\n",
      "|summary| Name| Age|\n",
      "+-------+-----+----+\n",
      "|  count|    3|   3|\n",
      "|   mean| null|30.0|\n",
      "| stddev| null| 1.0|\n",
      "|    min|hinat|  29|\n",
      "|    max| suki|  31|\n",
      "+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4db2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding columns in dataframe\n",
    "df_pyspark = df_pyspark.withColumn('Exp', df_pyspark['Age']+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa18ca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "| Name|Age|Exp|\n",
      "+-----+---+---+\n",
      "|hinat| 31| 41|\n",
      "| suki| 30| 40|\n",
      "| momo| 29| 39|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d3c53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop columns\n",
    "df_pyspark = df_pyspark.drop('Exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d68325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e99c0537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|Fname|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### rename columns\n",
    "df_pyspark.withColumnRenamed('Name', 'Fname').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f2c7f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964e4fd",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7804177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7861640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Missing value').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7161126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', sep = ';', header = True, inferSchema = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4ceb8ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "| Age| Exp|Salary|\n",
      "+----+----+------+\n",
      "|  31|  10| 30000|\n",
      "|  30|   8| 25000|\n",
      "|  29|   4| 20000|\n",
      "|  24|   3| 20000|\n",
      "|  21|   1| 15000|\n",
      "|  23|   2| 18000|\n",
      "|null|null| 40000|\n",
      "|  34|  10| 38000|\n",
      "|  36|null|  null|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52fa8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb19717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b1cdc35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## any == how\n",
    "df_pyspark.na.drop(how = 'any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6fb41238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold of 2 \n",
    "df_pyspark.na.drop(how = 'any', thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "232a0a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+------+\n",
      "| Name|Age|Exp|Salary|\n",
      "+-----+---+---+------+\n",
      "|hinat| 31| 10| 30000|\n",
      "| suki| 30|  8| 25000|\n",
      "| momo| 29|  4| 20000|\n",
      "|  zum| 24|  3| 20000|\n",
      "| maki| 21|  1| 15000|\n",
      "| null| 23|  2| 18000|\n",
      "| null| 34| 10| 38000|\n",
      "+-----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## subset - drops null values from 'Exp'\n",
    "df_pyspark.na.drop(how = 'any', subset = ['Exp']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7b2fe60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----+------+\n",
      "|         Name| Age| Exp|Salary|\n",
      "+-------------+----+----+------+\n",
      "|        hinat|  31|  10| 30000|\n",
      "|         suki|  30|   8| 25000|\n",
      "|         momo|  29|   4| 20000|\n",
      "|          zum|  24|   3| 20000|\n",
      "|         maki|  21|   1| 15000|\n",
      "|Missing value|  23|   2| 18000|\n",
      "|          avo|null|null| 40000|\n",
      "|Missing value|  34|  10| 38000|\n",
      "|Missing value|  36|null|  null|\n",
      "+-------------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### fill missing value\n",
    "df_pyspark.na.fill({'Name': 'Missing value'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bf3c6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+\n",
      "| Name| Age| Exp|Salary|\n",
      "+-----+----+----+------+\n",
      "|hinat|  31|  10| 30000|\n",
      "| suki|  30|   8| 25000|\n",
      "| momo|  29|   4| 20000|\n",
      "|  zum|  24|   3| 20000|\n",
      "| maki|  21|   1| 15000|\n",
      "| null|  23|   2| 18000|\n",
      "|  avo|null|null| 40000|\n",
      "| null|  34|  10| 38000|\n",
      "| null|  36|null|  null|\n",
      "+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0f44b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Exp', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Exp', 'Salary']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e21602d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "| Name| Age| Exp|Salary|Age_imputed|Exp_imputed|Salary_imputed|\n",
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "|hinat|  31|  10| 30000|         31|         10|         30000|\n",
      "| suki|  30|   8| 25000|         30|          8|         25000|\n",
      "| momo|  29|   4| 20000|         29|          4|         20000|\n",
      "|  zum|  24|   3| 20000|         24|          3|         20000|\n",
      "| maki|  21|   1| 15000|         21|          1|         15000|\n",
      "| null|  23|   2| 18000|         23|          2|         18000|\n",
      "|  avo|null|null| 40000|         28|          5|         40000|\n",
      "| null|  34|  10| 38000|         34|         10|         38000|\n",
      "| null|  36|null|  null|         36|          5|         25750|\n",
      "+-----+----+----+------+-----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "63d416f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F086109630>>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d331d8c",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "62470f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Filter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0837cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# got tired of using df_pyspark\n",
    "df = spark.read.csv('test1.csv', header = True, inferSchema = True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f58f2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|hinat| 31|\n",
      "| suki| 30|\n",
      "| momo| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f6402a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|suki| 30|\n",
      "|momo| 29|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## cats age less than 30\n",
    "df.filter(\"Age<=30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "59b647eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|suki|\n",
      "|momo|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Age<=30\").select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "de34ccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df['Age']<=30) & (df['Name'] == 'suki')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c87ff2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001F091747BE0>>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d6d2a",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4882989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Group').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35031ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2KVSSK2L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Group</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a216c4c4c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb22060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('test3.csv', header = True, inferSchema = True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7d6695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+\n",
      "| Name|Departments|Salary|\n",
      "+-----+-----------+------+\n",
      "|hinat|         DS| 10000|\n",
      "| suki|        IOT|  5000|\n",
      "| momo|         BD|  4000|\n",
      "|  zum|         BD|  4000|\n",
      "| maki|         DS|  3000|\n",
      "|bussy|         DS| 20000|\n",
      "|  avo|        IOT| 10000|\n",
      "|Sunny|         BD|  5000|\n",
      "|Sunny|         DS| 10000|\n",
      "|Krish|         BD|  2000|\n",
      "+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f74ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f9c7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|sum(Salary)|\n",
      "+-----+-----------+\n",
      "| suki|       5000|\n",
      "|Sunny|      15000|\n",
      "|  zum|       4000|\n",
      "|Krish|       2000|\n",
      "|bussy|      20000|\n",
      "| maki|       3000|\n",
      "| momo|       4000|\n",
      "|hinat|      10000|\n",
      "|  avo|      10000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "df.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584f0310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|Departments|sum(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOT|      15000|\n",
      "|         BD|      15000|\n",
      "|         DS|      43000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby departments\n",
    "df.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02bd9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|Departments|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "|        IOT|     7500.0|\n",
      "|         BD|     3750.0|\n",
      "|         DS|    10750.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609aec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|Departments|count|\n",
      "+-----------+-----+\n",
      "|        IOT|    2|\n",
      "|         BD|    4|\n",
      "|         DS|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3723889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852b78a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|min(Salary)|\n",
      "+-----+-----------+\n",
      "| suki|       5000|\n",
      "|Sunny|       5000|\n",
      "|  zum|       4000|\n",
      "|Krish|       2000|\n",
      "|bussy|      20000|\n",
      "| maki|       3000|\n",
      "| momo|       4000|\n",
      "|hinat|      10000|\n",
      "|  avo|      10000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8abbe27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001A216C4C4C0>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f045dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
